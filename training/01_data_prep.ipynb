{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f16e594-1ba8-4caf-8127-30d3a51c5790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qqqU sagemaker wandb datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0730f66a-6fca-43c9-86d6-5c97804c3588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sagemaker-llama2-wandb/training/wandb/run-20231011_214651-03xbew1p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/sagemaker-llama2-wandb-training/runs/03xbew1p' target=\"_blank\">zesty-pine-2</a></strong> to <a href='https://wandb.ai/capecape/sagemaker-llama2-wandb-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/sagemaker-llama2-wandb-training' target=\"_blank\">https://wandb.ai/capecape/sagemaker-llama2-wandb-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/sagemaker-llama2-wandb-training/runs/03xbew1p' target=\"_blank\">https://wandb.ai/capecape/sagemaker-llama2-wandb-training/runs/03xbew1p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('parambharat/wandbot_eval/eval_results:latest', type='dataset')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853cddf4-e58b-4871-88af-1f801550acc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "table = artifact.get(\"eval_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07de4ea0-b533-4755-a90f-0d4a5fe51379",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question',\n",
       " 'answer',\n",
       " 'reaction',\n",
       " 'contexts',\n",
       " 'answer_relevancy',\n",
       " 'context_relavency',\n",
       " 'faithfulness',\n",
       " 'correctness']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9003452-69f0-484a-84ec-d19ef3f4fa8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(table.data, columns=table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712996dd-f8d3-483c-8bb8-abda7360cac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df.reaction!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9b13c2-aac1-48c5-8c4a-a95cd63cfb23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>reaction</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_relavency</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>write sample code to upload a table to a run, ...</td>\n",
       "      <td>Sure, here is how you can upload a table to a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[description: Explore how to use W&amp;B Tables wi...</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>write sample code to upload a table to a run, ...</td>\n",
       "      <td>Sure, here is how you can upload a table to a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[description: How to use the sidebar and table...</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is there an integration with MONAI?</td>\n",
       "      <td>Hmm, I'm not sure. The provided documentation ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Integrations\\n\\nView source on GitHub\\n\\nModu...</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how do I resume a run and attach an artifact t...</td>\n",
       "      <td>**Warning: Falling back to gpt-3.5.** These re...</td>\n",
       "      <td>0</td>\n",
       "      <td>[```\\n\\nResume Guidance\\n\\nThere are different...</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>can i attach an artifact to an already existin...</td>\n",
       "      <td>Yes, you can attach an artifact to an already ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[```\\n\\n✔️ run.use_artifact\\n\\nThese steps are...</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  write sample code to upload a table to a run, ...   \n",
       "1  write sample code to upload a table to a run, ...   \n",
       "2                is there an integration with MONAI?   \n",
       "4  how do I resume a run and attach an artifact t...   \n",
       "5  can i attach an artifact to an already existin...   \n",
       "\n",
       "                                              answer  reaction  \\\n",
       "0  Sure, here is how you can upload a table to a ...         0   \n",
       "1  Sure, here is how you can upload a table to a ...         0   \n",
       "2  Hmm, I'm not sure. The provided documentation ...         0   \n",
       "4  **Warning: Falling back to gpt-3.5.** These re...         0   \n",
       "5  Yes, you can attach an artifact to an already ...         0   \n",
       "\n",
       "                                            contexts  answer_relevancy  \\\n",
       "0  [description: Explore how to use W&B Tables wi...             0.897   \n",
       "1  [description: How to use the sidebar and table...             0.884   \n",
       "2  [Integrations\\n\\nView source on GitHub\\n\\nModu...             0.884   \n",
       "4  [```\\n\\nResume Guidance\\n\\nThere are different...             0.911   \n",
       "5  [```\\n\\n✔️ run.use_artifact\\n\\nThese steps are...             0.850   \n",
       "\n",
       "   context_relavency  faithfulness  correctness  \n",
       "0                0.0      0.333333            1  \n",
       "1                0.0      1.000000            1  \n",
       "2                0.0      0.000000            1  \n",
       "4                0.0      0.500000            1  \n",
       "5                0.0      0.000000            1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b3b428-c63e-4261-9b0f-147e9e022841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c13916-758c-43a2-98d9-6f12bc3ceb14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[245][\"contexts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5942070f-a843-4486-bc7e-a30b8aff856a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_template(row):\n",
    "    return (\"You are an AI assistant designed to assist developers with everyday tasks related to Weight & Biases \"\n",
    "            \"and provide helpful information. As an expert in the open-source python SDK wandb answer the following\"\n",
    "            \"question based on the context. Answer in formatted Markdown.\\n\"\n",
    "            \"Write a response that appropriately answer the question\\n\\n\"\n",
    "            \"### Question:\\n{question}\\n\\n### Response:\\n{answer}\").format_map(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "384afe6b-94eb-4fff-b7ee-c62219e39d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an AI assistant designed to assist developers with everyday tasks related to Weight & Biases and provide helpful information. As an expert in the open-source python SDK wandb answer the followingquestion based on the context. Answer in formatted Markdown.\\nWrite a response that appropriately answer the question\\n\\n### Question:\\nwrite sample code to upload a table to a run, then download that table again in a new run as a Pandas Dataframe\\n\\n### Response:\\nSure, here is how you can upload a table to a run and then download it in a new run as a Pandas DataFrame:\\n\\nFirst, let\\'s create a table and log it to a run:\\n\\n```python\\nimport wandb\\n\\n# Initialize a new run\\nrun = wandb.init(project=\"table-test\")\\nmy_table = wandb.Table(\\n    columns=[\"a\", \"b\"], \\n    data=[[\"a1\", \"b1\"], [\"a2\", \"b2\"]]\\n    )\\nrun.log({\"Table Name\": my_table})\\n```\\n\\nNow, let\\'s download the table in a new run:\\n\\n```python\\nimport wandb\\n\\n# Initialize a new run\\nrun = wandb.init(project=\"table-test\")\\n\\n# Use the artifact API to access the table\\nartifact = run.use_artifact(\"run-<run-id>-Table Name:<tag>\")\\ntable = artifact.get(\"Table Name\")\\n\\n# Convert the table to a Pandas DataFrame\\ndf = table.to_pandas()\\n```\\n\\nPlease replace `<run-id>` and `<tag>` with the actual run ID and tag. The run ID can be found in the URL of the run page in the Weights & Biases web interface. The tag is the version of the artifact, which is usually a number.\\n\\nFor more information, you can refer to the [wandb documentation](<https://docs.wandb.ai/>).'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2ffe6-9ec8-4ab0-8d7e-8594277d0431",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae08b1-5519-43e4-ba28-099950a0bd63",
   "metadata": {},
   "source": [
    "If you use CodeLLama we need to format the instructions accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "629ce7ab-12b2-43c8-9fe8-19cdf55865cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_contexts(ctxs):\n",
    "    \"The model context is too short!!!\"\n",
    "    return ctxs[0]  # too long!\n",
    "    # return \"\\n\".join(ctxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbb5948d-1b37-46b6-b6e1-5b1c65498bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST] \", \" [/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "EOS = \"</s>\"\n",
    "\n",
    "\n",
    "def format_text(row):\n",
    "    text = (\n",
    "        B_INST\n",
    "        + B_SYS\n",
    "        + \"You are an AI assistant designed to assist developers with everyday tasks related to Weight & Biases\"\n",
    "        + \"and provide helpful information. As an expert in the open-source python SDK wandb answer the following\"\n",
    "        # + \"question based on the context below. Answer in formatted Markdown.\\n\"\n",
    "        + \"question below. Answer in formatted Markdown.\\n\"\n",
    "        # + merge_contexts(row[\"contexts\"])\n",
    "        + E_SYS\n",
    "        + row[\"question\"]\n",
    "        + E_INST\n",
    "        + \"\\n[W&B]\\n\"\n",
    "        + row[\"answer\"]\n",
    "        + \"\\n[/W&B]\"\n",
    "        + EOS\n",
    "    )\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9741054-c154-416c-998d-54385224cf7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are an AI assistant designed to assist developers with everyday tasks related to Weight & Biasesand provide helpful information. As an expert in the open-source python SDK wandb answer the followingquestion below. Answer in formatted Markdown.\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "write sample code to upload a table to a run, then download that table again in a new run as a Pandas Dataframe [/INST]\n",
      "[W&B]\n",
      "Sure, here is how you can upload a table to a run and then download it in a new run as a Pandas DataFrame:\n",
      "\n",
      "First, let's create a table and log it to a run:\n",
      "\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "# Initialize a new run\n",
      "run = wandb.init(project=\"table-test\")\n",
      "my_table = wandb.Table(\n",
      "    columns=[\"a\", \"b\"], \n",
      "    data=[[\"a1\", \"b1\"], [\"a2\", \"b2\"]]\n",
      "    )\n",
      "run.log({\"Table Name\": my_table})\n",
      "```\n",
      "\n",
      "Now, let's download the table in a new run:\n",
      "\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "# Initialize a new run\n",
      "run = wandb.init(project=\"table-test\")\n",
      "\n",
      "# Use the artifact API to access the table\n",
      "artifact = run.use_artifact(\"run-<run-id>-Table Name:<tag>\")\n",
      "table = artifact.get(\"Table Name\")\n",
      "\n",
      "# Convert the table to a Pandas DataFrame\n",
      "df = table.to_pandas()\n",
      "```\n",
      "\n",
      "Please replace `<run-id>` and `<tag>` with the actual run ID and tag. The run ID can be found in the URL of the run page in the Weights & Biases web interface. The tag is the version of the artifact, which is usually a number.\n",
      "\n",
      "For more information, you can refer to the [wandb documentation](<https://docs.wandb.ai/>).\n",
      "[/W&B]</s>\n"
     ]
    }
   ],
   "source": [
    "one_example = format_text(df.iloc[0])\n",
    "print(one_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bddd870b-02a3-4628-9cba-a1256cb3e9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    dataset.append({\"text\": format_text(row)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de57e448-f85c-49e6-85c4-87ffb82f9e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aedef108-1385-4f94-b493-409d64a1577e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_jsonl(dataset, \"wandb_questions_ds.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1017dd8-f56d-4dfb-90ef-ddf604c75e8f",
   "metadata": {},
   "source": [
    "## SageMaker auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee341747-68ae-4b11-ab7c-2e5b86bf89ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::372108735839:role/SageMakerExecutionRole\n",
      "sagemaker bucket: sagemaker-us-east-1-372108735839\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebe295-f927-406f-b790-27bc059abab1",
   "metadata": {},
   "source": [
    "## Preparing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5380e7bd-66f3-4237-a901-fe935a249982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"codellama/CodeLlama-7b-Instruct-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20aaffb3-d181-4c34-a58d-3e9c78a16d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a3eec28-6674-4b44-a4ba-4f2c2f44be89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d894909-a161-4806-832f-0da72a580860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a346c118e92f4f06ba645902a3aebe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75140e9dcb144ca5a02479f3e3f48379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a7cbc6354348f19cf66a7dfbc7e24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 811\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(path=\".\", data_files=\"wandb_questions_ds.jsonl\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a4aa0e2-c9b3-4065-8c4a-ca24f8bfd856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '[INST] <<SYS>>\\nYou are an AI assistant designed to assist developers with everyday tasks related to Weight & Biasesand provide helpful information. As an expert in the open-source python SDK wandb answer the followingquestion below. Answer in formatted Markdown.\\n\\n<</SYS>>\\n\\nwrite sample code to upload a table to a run, then download that table again in a new run as a Pandas Dataframe [/INST]\\n[W&B]\\nSure, here is how you can upload a table to a run and then download it in a new run as a Pandas DataFrame:\\n\\nFirst, let\\'s create a table and log it to a run:\\n\\n```python\\nimport wandb\\n\\n# Initialize a new run\\nrun = wandb.init(project=\"table-test\")\\nmy_table = wandb.Table(\\n    columns=[\"a\", \"b\"], \\n    data=[[\"a1\", \"b1\"], [\"a2\", \"b2\"]]\\n    )\\nrun.log({\"Table Name\": my_table})\\n```\\n\\nNow, let\\'s download the table in a new run:\\n\\n```python\\nimport wandb\\n\\n# Initialize a new run\\nrun = wandb.init(project=\"table-test\")\\n\\n# Use the artifact API to access the table\\nartifact = run.use_artifact(\"run-<run-id>-Table Name:<tag>\")\\ntable = artifact.get(\"Table Name\")\\n\\n# Convert the table to a Pandas DataFrame\\ndf = table.to_pandas()\\n```\\n\\nPlease replace `<run-id>` and `<tag>` with the actual run ID and tag. The run ID can be found in the URL of the run page in the Weights & Biases web interface. The tag is the version of the artifact, which is usually a number.\\n\\nFor more information, you can refer to the [wandb documentation](<https://docs.wandb.ai/>).\\n[/W&B]</s>'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac52401-ea01-44bb-95d7-b11f8aa34b59",
   "metadata": {},
   "source": [
    "We define some helper functions to pack our samples into sequences of a given length and then tokenize them.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0eb36f2-6dbf-4f13-b8ea-1ba2effc709c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c8e6a956804ed1941c901c8ae1274e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '[INST] <<SYS>>\\nYou are an AI assistant designed to assist developers with everyday tasks related to Weight & Biasesand provide helpful information. As an expert in the open-source python SDK wandb answer the followingquestion below. Answer in formatted Markdown.\\n\\n<</SYS>>\\n\\nwhere can I find info about my weights & biases license? [/INST]\\n[W&B]\\nYou can find information about your Weights & Biases account, including storage usage, in your user settings. To access your user settings, navigate to your user profile page and select your user icon on the top right corner. From the dropdown, choose \"Settings.\"\\n\\nFor information about storage and pricing, you can visit the [Pricing page](<https://wandb.ai/site/pricing>).\\n\\nPlease note that I cannot provide a direct link to your user settings, as it requires you to be logged in to your Weights & Biases account.\\n\\n*References*:\\n\\n>- <https://docs.wandb.ai/guides/app/settings-page/user-settings>\\n>- <https://docs.wandb.ai/guides/integrations/huggingface>\\n>- <https://docs.wandb.ai/guides/integrations/add-wandb-to-any-library>\\n>- <https://docs.wandb.ai/guides>\\n>- <https://docs.wandb.ai/guides/integrations/metaflow>\\n[/W&B]</s>'}</s>\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{sample}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset)\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec8e2766-5b3a-4424-82e6-1cf5ef45da1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a70887888248fea5746c670a9e508f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c29ef5b9de4b24bfac0d256ca0426e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 304\n"
     ]
    }
   ],
   "source": [
    "def chunk(sample, chunk_length=1024):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=1024),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0825677-00f9-4447-94af-eeae01852f03",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pretty small =P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4726a-5381-4a56-8e5a-59fdfffb8c7c",
   "metadata": {},
   "source": [
    "## Save to a bucket and W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "322268d9-80a8-4f67-bf55-76cf4773ca95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8573b458d04c4b9aa53ed9ef9deb4aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-372108735839/processed/wandbot/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/wandbot/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f92d3c9a-f01d-4b6e-8a20-ab707918a0e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "at = wandb.Artifact(\n",
    "    name=\"wandbot_dataset\", \n",
    "    type=\"dataset\",\n",
    "    description=\"A wandbot dataset of questions and answers about W&B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7edcf658-cd05-46de-bc30-45c27a8f4853",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:03xbew1p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-pine-2</strong> at: <a href='https://wandb.ai/capecape/sagemaker-llama2-wandb-training/runs/03xbew1p' target=\"_blank\">https://wandb.ai/capecape/sagemaker-llama2-wandb-training/runs/03xbew1p</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231011_214651-03xbew1p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:03xbew1p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78050133449745209afe53fbb71b6346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111323939015468, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sagemaker-llama2-wandb/training/wandb/run-20231011_214725-wjgb7hk4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/aws_llm_demo/runs/wjgb7hk4' target=\"_blank\">fiery-lake-3</a></strong> to <a href='https://wandb.ai/capecape/aws_llm_demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/aws_llm_demo' target=\"_blank\">https://wandb.ai/capecape/aws_llm_demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/aws_llm_demo/runs/wjgb7hk4' target=\"_blank\">https://wandb.ai/capecape/aws_llm_demo/runs/wjgb7hk4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Generating checksum for up to 10000 objects in \"sagemaker-us-east-1-372108735839/processed/wandbot/train\"... Done. 0.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cd7ea5bf784311b0836952fd0671ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.050 MB of 0.050 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-lake-3</strong> at: <a href='https://wandb.ai/capecape/aws_llm_demo/runs/wjgb7hk4' target=\"_blank\">https://wandb.ai/capecape/aws_llm_demo/runs/wjgb7hk4</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231011_214725-wjgb7hk4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"aws_llm_demo\"):\n",
    "    at.add_reference(training_input_path)\n",
    "    wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68e98a-6297-40d4-a337-e033b48311e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc66ee4-46ac-40c6-bf69-344e0de9eeda",
   "metadata": {},
   "source": [
    "We need to save our W&B credentials on the folder that will be copied to the SM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bedc356b-87db-4e84-9513-2737b3a487d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.sagemaker_auth(path=\"scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d89cb47-7099-47af-a76a-0b7a83e966f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "523fd2a0-e395-4b29-be66-deb0d8b9d48b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.34.0'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1db0562a-31eb-46d7-a7a8-46fc7307cbf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-codellama7'\n",
    "\n",
    "lr = 2e-4\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "    'model_id': MODEL_NAME,                             # pre-trained model\n",
    "    'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "    'epochs': 1,                                      # number of training epochs\n",
    "    'per_device_train_batch_size': 2,                 # batch size for training\n",
    "    'lr': lr,                                       # learning rate used during training\n",
    "    'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "    'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "    'report_to': \"wandb\",                              # report to wandb\n",
    "    'wandb_project': \"aws_llm_demo\",\n",
    "    \"run_name\":  f\"{MODEL_NAME}__qlora\",\n",
    "}\n",
    "    \n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\"}, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6e4a84a-f656-4898-8f79-829c307c0e70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported huggingface version: 4.34. You may need to upgrade your SDK version (pip install -U sagemaker) for newer huggingface versions. Supported huggingface version(s): 4.4.2, 4.5.0, 4.6.1, 4.10.2, 4.11.0, 4.12.3, 4.17.0, 4.26.0, 4.28.1, 4.4, 4.5, 4.6, 4.10, 4.11, 4.12, 4.17, 4.26, 4.28.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m: training_input_path}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# starting the train job with our uploaded datasets as input\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1311\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1310\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2372\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_new\u001b[39m(\u001b[38;5;28mcls\u001b[39m, estimator, inputs, experiment_config):\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[1;32m   2350\u001b[0m \n\u001b[1;32m   2351\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;124;03m        all information about the started training job.\u001b[39;00m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2372\u001b[0m     train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_train_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2374\u001b[0m     estimator\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_args)\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(estimator\u001b[38;5;241m.\u001b[39msagemaker_session, estimator\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2479\u001b[0m, in \u001b[0;36m_TrainingJob._get_train_args\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2477\u001b[0m     train_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm_arn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39malgorithm_arn\n\u001b[1;32m   2478\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2479\u001b[0m     train_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_image_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mdebugger_rule_configs:\n\u001b[1;32m   2482\u001b[0m     train_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebugger_rule_configs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mdebugger_rule_configs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:3599\u001b[0m, in \u001b[0;36mFramework.training_image_uri\u001b[0;34m(self, region)\u001b[0m\n\u001b[1;32m   3584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_image_uri\u001b[39m(\u001b[38;5;28mself\u001b[39m, region\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3585\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Docker image to use for training.\u001b[39;00m\n\u001b[1;32m   3586\u001b[0m \n\u001b[1;32m   3587\u001b[0m \u001b[38;5;124;03m    The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3596\u001b[0m \u001b[38;5;124;03m        str: The URI of the Docker image.\u001b[39;00m\n\u001b[1;32m   3597\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage_uris\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_training_image_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_region_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_framework_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=no-member\u001b[39;49;00m\n\u001b[1;32m   3603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpy_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=no-member\u001b[39;49;00m\n\u001b[1;32m   3604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistribution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompiler_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorflow_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtensorflow_version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpytorch_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch_version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3609\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_instance_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/utilities.py:417\u001b[0m, in \u001b[0;36moverride_pipeline_parameter_var.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(warning_msg_template, arg_name, func_name, \u001b[38;5;28mtype\u001b[39m(value))\n\u001b[1;32m    416\u001b[0m         kwargs[arg_name] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mdefault_value\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/image_uris.py:663\u001b[0m, in \u001b[0;36mget_training_image_uri\u001b[0;34m(region, framework, framework_version, py_version, image_uri, distribution, compiler_config, tensorflow_version, pytorch_version, instance_type)\u001b[0m\n\u001b[1;32m    660\u001b[0m     container_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     base_framework_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpy_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpy_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_framework_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_framework_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_compiler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/utilities.py:417\u001b[0m, in \u001b[0;36moverride_pipeline_parameter_var.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(warning_msg_template, arg_name, func_name, \u001b[38;5;28mtype\u001b[39m(value))\n\u001b[1;32m    416\u001b[0m         kwargs[arg_name] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mdefault_value\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/image_uris.py:176\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config, sagemaker_session)\u001b[0m\n\u001b[1;32m    173\u001b[0m     config \u001b[38;5;241m=\u001b[39m _config_for_framework_and_scope(_framework, final_image_scope, accelerator_type)\n\u001b[1;32m    175\u001b[0m original_version \u001b[38;5;241m=\u001b[39m version\n\u001b[0;32m--> 176\u001b[0m version \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_version_and_set_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m version_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversions\u001b[39m\u001b[38;5;124m\"\u001b[39m][_version_for_config(version, config)]\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m HUGGING_FACE_FRAMEWORK:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/image_uris.py:473\u001b[0m, in \u001b[0;36m_validate_version_and_set_if_needed\u001b[0;34m(version, config, framework)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m framework \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    467\u001b[0m     DATA_WRANGLER_FRAMEWORK,\n\u001b[1;32m    468\u001b[0m     HUGGING_FACE_LLM_FRAMEWORK,\n\u001b[1;32m    469\u001b[0m     STABILITYAI_FRAMEWORK,\n\u001b[1;32m    470\u001b[0m ]:\n\u001b[1;32m    471\u001b[0m     version \u001b[38;5;241m=\u001b[39m _get_latest_versions(available_versions)\n\u001b[0;32m--> 473\u001b[0m \u001b[43m_validate_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_versions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maliased_versions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m version\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/image_uris.py:585\u001b[0m, in \u001b[0;36m_validate_arg\u001b[0;34m(arg, available_options, arg_name)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks if the arg is in the available options, and raises a ``ValueError`` if not.\"\"\"\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_options:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported \u001b[39m\u001b[38;5;132;01m{arg_name}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{arg}\u001b[39;00m\u001b[38;5;124m. You may need to upgrade your SDK version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(pip install -U sagemaker) for newer \u001b[39m\u001b[38;5;132;01m{arg_name}\u001b[39;00m\u001b[38;5;124ms. Supported \u001b[39m\u001b[38;5;132;01m{arg_name}\u001b[39;00m\u001b[38;5;124m(s): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{options}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arg_name\u001b[38;5;241m=\u001b[39marg_name, arg\u001b[38;5;241m=\u001b[39marg, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(available_options))\n\u001b[1;32m    589\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported huggingface version: 4.34. You may need to upgrade your SDK version (pip install -U sagemaker) for newer huggingface versions. Supported huggingface version(s): 4.4.2, 4.5.0, 4.6.1, 4.10.2, 4.11.0, 4.12.3, 4.17.0, 4.26.0, 4.28.1, 4.4, 4.5, 4.6, 4.10, 4.11, 4.12, 4.17, 4.26, 4.28."
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d2bf8-a92e-48e5-9ac4-b04fe5b13675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
