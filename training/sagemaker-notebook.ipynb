{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "\n",
    "In this sagemaker example, we are going to learn how to fine-tune [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) using [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314). [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) is the next version of the [LLaMA](https://arxiv.org/abs/2302.13971). Compared to the V1 model, it is trained on more data - 2T tokens and supports context length window upto 4K tokens. Learn more about LLaMa 2 in the [\"\"]() blog post.\n",
    "\n",
    "QLoRA is an efficient finetuning technique that quantizes a pretrained language model to 4 bits and attaches small “Low-Rank Adapters” which are fine-tuned. This enables fine-tuning of models with up to 65 billion parameters on a single GPU; despite its efficiency, QLoRA matches the performance of full-precision fine-tuning and achieves state-of-the-art results on language tasks.\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "In Detail you will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "4. Deploy Fine-tuned LLM on Amazon SageMaker\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- (Q)LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- IA3: [Infused Adapter by Inhibiting and Amplifying Inner Activations](https://arxiv.org/abs/2205.05638)\n",
    "\n",
    "\n",
    "\n",
    "### Access LLaMA 2\n",
    "\n",
    "Before we can start training we have to make sure that we accepted the license of [llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) to be able to use it. You can accept the license by clicking on the Agree and access repository button on the model page at: \n",
    "* [LLaMa 7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "* [LLaMa 13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)\n",
    "* [LLaMa 70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate \"datasets[s3]==2.13.0\" sagemaker wandb --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any LLaMA 2 asset we need to login into our hugging face account. We can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"your-hf-token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights & Biases Setup for AWS SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only additional piece of setup needed to use W&B with SageMaker is to make your W&B API key available to SageMaker. In this case we save it to a file in the same directory as our training script. This will be named secrets.env and W&B will then use this to authenticate on each of the instances that SageMaker spins up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.sagemaker_auth(path=\"scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::372108735839:role/SageMakerExecutionRole\n",
      "sagemaker bucket: sagemaker-us-east-1-372108735839\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "we will use the [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `samsum` dataset, we use the `load_dataset()` method from the 🤗 Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 8.20k/8.20k [00:00<00:00, 2.60MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/databricks--databricks-dolly-15k to /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 13.1M/13.1M [00:00<00:00, 110MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 33.36it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "dataset size: 15011\n",
      "{'instruction': 'Provide me a list of some ways to get exercise that are fun', 'context': '', 'response': 'Here are a few of the ways you can exercise that are fun: dancing, playing frisbee, playing sports, playing with a pet or a child, skiing, or swimming in a lake.', 'category': 'brainstorming'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Why do cats make purring sounds?\n",
      "\n",
      "### Answer\n",
      "Cats make purring sounds as a way to communicate emotions which may include relaxation, happiness, and sometimes even excitement. However, the exact reason why cats make the purring sounds has not been fully understood.\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to pack our samples into sequences of a given length and then tokenize them.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What equipment is required for scuba diving?\n",
      "\n",
      "### Answer\n",
      "Scuba diving requires\n",
      "* An air (or enriched air) tank\n",
      "* Breathing device - typically an open circuit regulator  or a closed circuit rebreather\n",
      "* Buoyancy Control Device (bcd)\n",
      "* Depth gauge\n",
      "* Submersible Pressure Gauge\n",
      "* Dive Computer or Watch\n",
      "* Mask\n",
      "* Fins\n",
      "Optional equipment\n",
      "* Wetsuit\n",
      "* Weights\n",
      "* Surface Marker Buoy\n",
      "* Dive Light</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-372108735839/processed/llama/dolly/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "### Harwarde requirements\n",
    "\n",
    "We also ran several experiments to determine, which instance type can be used for the different model sizes. The following table shows the results of our experiments. The table shows the instance type, model size, context length, and max batch size. \n",
    "\n",
    "| Model        | Instance Type     | Max Batch Size | Context Length |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training.\n",
    "\n",
    "_Note: We plan to extend this list in the future. feel free to contribute your setup!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "lr = 2e-4\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 1,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': lr,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "  'report_to': \"wandb\",                              # report to wandb\n",
    "}\n",
    "\n",
    "hyperparameters['run_name'] = f\"{model_id}_{lr}_qlora\"\n",
    "    \n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    # instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\"}, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 14:21:13 Starting - Starting the training job...\n",
      "2023-08-31 14:21:28 Starting - Preparing the instances for training......\n",
      "2023-08-31 14:22:43 Downloading - Downloading input data.........\n",
      "2023-08-31 14:24:13 Training - Downloading the training image.........\n",
      "2023-08-31 14:25:34 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:42,496 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:42,509 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:42,518 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:42,520 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:43,836 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 54.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 17.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 33.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.40.2 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 13.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 84.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.13.3)\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.15.9-py3-none-any.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 84.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 7)) (8.1.3)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.5/188.5 kB 32.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.30.0-py2.py3-none-any.whl (218 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 218.8/218.8 kB 30.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools (from wandb->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 7)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 7)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 7)) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pathtools\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=90f1cb1f9cf3b3ea0858e90221c9a4e8ce921c374bb4692228eb39c0681f9fc2\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\u001b[0m\n",
      "\u001b[34mSuccessfully built pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, pathtools, bitsandbytes, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, transformers, GitPython, accelerate, wandb, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.32 accelerate-0.21.0 bitsandbytes-0.40.2 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 peft-0.4.0 safetensors-0.3.3 sentry-sdk-1.30.0 setproctitle-1.3.2 smmap-5.0.0 transformers-4.31.0 wandb-0.15.9\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:57,574 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:57,575 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:57,589 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:57,612 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:57,634 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:57,645 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 1,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"lr\": 0.0002,\n",
      "        \"merge_weights\": true,\n",
      "        \"model_id\": \"meta-llama/Llama-2-7b-hf\",\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"report_to\": \"wandb\",\n",
      "        \"run_name\": \"meta-llama/Llama-2-7b-hf_0.0002_qlora\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-372108735839/huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"hf_token\":\"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\",\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"per_device_train_batch_size\":2,\"report_to\":\"wandb\",\"run_name\":\"meta-llama/Llama-2-7b-hf_0.0002_qlora\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-372108735839/huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"hf_token\":\"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\",\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"per_device_train_batch_size\":2,\"report_to\":\"wandb\",\"run_name\":\"meta-llama/Llama-2-7b-hf_0.0002_qlora\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-372108735839/huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"1\",\"--hf_token\",\"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\",\"--lr\",\"0.0002\",\"--merge_weights\",\"True\",\"--model_id\",\"meta-llama/Llama-2-7b-hf\",\"--per_device_train_batch_size\",\"2\",\"--report_to\",\"wandb\",\"--run_name\",\"meta-llama/Llama-2-7b-hf_0.0002_qlora\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_WEIGHTS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=meta-llama/Llama-2-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO=wandb\u001b[0m\n",
      "\u001b[34mSM_HP_RUN_NAME=meta-llama/Llama-2-7b-hf_0.0002_qlora\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 1 --hf_token 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 --lr 0.0002 --merge_weights True --model_id meta-llama/Llama-2-7b-hf --per_device_train_batch_size 2 --report_to wandb --run_name meta-llama/Llama-2-7b-hf_0.0002_qlora\u001b[0m\n",
      "\u001b[34m2023-08-31 14:26:57,671 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mLogging into the Hugging Face Hub with token hf_hlzNMcL...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid.\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 609/609 [00:00<00:00, 7.06MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 189MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:26, 381MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:26, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:26, 365MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 168M/9.98G [00:00<00:26, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 220M/9.98G [00:00<00:24, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 273M/9.98G [00:00<00:23, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 315M/9.98G [00:00<00:23, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▎         | 357M/9.98G [00:00<00:23, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 409M/9.98G [00:01<00:22, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▍         | 461M/9.98G [00:01<00:22, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▌         | 514M/9.98G [00:01<00:24, 381MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 556M/9.98G [00:01<00:28, 325MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 598M/9.98G [00:01<00:40, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▋         | 629M/9.98G [00:02<00:47, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 661M/9.98G [00:02<00:50, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 692M/9.98G [00:02<00:50, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 713M/9.98G [00:02<00:50, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 744M/9.98G [00:02<00:48, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 776M/9.98G [00:02<00:42, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 828M/9.98G [00:02<00:33, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▊         | 860M/9.98G [00:03<00:34, 267MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 902M/9.98G [00:03<00:30, 298MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 944M/9.98G [00:03<00:28, 318MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|▉         | 986M/9.98G [00:03<00:28, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 1.04G/9.98G [00:03<00:25, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.08G/9.98G [00:03<00:24, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█▏        | 1.13G/9.98G [00:03<00:22, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.18G/9.98G [00:03<00:21, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.24G/9.98G [00:03<00:20, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.29G/9.98G [00:04<00:27, 312MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.33G/9.98G [00:04<00:36, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▎        | 1.36G/9.98G [00:04<00:39, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 1.39G/9.98G [00:04<00:39, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 1.44G/9.98G [00:04<00:33, 257MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 1.49G/9.98G [00:05<00:28, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 1.54G/9.98G [00:05<00:24, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.59G/9.98G [00:05<00:23, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▋        | 1.64G/9.98G [00:05<00:24, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.69G/9.98G [00:05<00:22, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.74G/9.98G [00:05<00:20, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.79G/9.98G [00:05<00:19, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.85G/9.98G [00:05<00:20, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.90G/9.98G [00:06<00:19, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 1.95G/9.98G [00:06<00:19, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 2.00G/9.98G [00:06<00:18, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.06G/9.98G [00:06<00:18, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.11G/9.98G [00:06<00:17, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.16G/9.98G [00:06<00:17, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.21G/9.98G [00:06<00:17, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.26G/9.98G [00:06<00:17, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.32G/9.98G [00:06<00:17, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.37G/9.98G [00:07<00:16, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.42G/9.98G [00:07<00:16, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▍       | 2.47G/9.98G [00:07<00:16, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 2.53G/9.98G [00:07<00:16, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.58G/9.98G [00:07<00:16, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▋       | 2.63G/9.98G [00:07<00:15, 461MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.68G/9.98G [00:07<00:15, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.74G/9.98G [00:07<00:15, 462MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.79G/9.98G [00:08<00:16, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.84G/9.98G [00:08<00:16, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.89G/9.98G [00:08<00:16, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 2.95G/9.98G [00:08<00:17, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|███       | 3.00G/9.98G [00:08<00:16, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.05G/9.98G [00:08<00:16, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.10G/9.98G [00:08<00:15, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.16G/9.98G [00:08<00:15, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.21G/9.98G [00:09<00:15, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.26G/9.98G [00:09<00:18, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.30G/9.98G [00:09<00:18, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▎      | 3.34G/9.98G [00:09<00:17, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 3.39G/9.98G [00:09<00:17, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 3.44G/9.98G [00:09<00:16, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▍      | 3.49G/9.98G [00:09<00:15, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.54G/9.98G [00:09<00:14, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.60G/9.98G [00:09<00:14, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.65G/9.98G [00:10<00:21, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.70G/9.98G [00:10<00:19, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.74G/9.98G [00:10<00:18, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.80G/9.98G [00:10<00:16, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▊      | 3.85G/9.98G [00:10<00:15, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.90G/9.98G [00:10<00:15, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 3.95G/9.98G [00:10<00:14, 414MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 4.01G/9.98G [00:11<00:14, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.06G/9.98G [00:11<00:13, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.11G/9.98G [00:11<00:13, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.16G/9.98G [00:11<00:13, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.22G/9.98G [00:11<00:13, 434MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.27G/9.98G [00:11<00:13, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.32G/9.98G [00:11<00:13, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.37G/9.98G [00:11<00:12, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.42G/9.98G [00:12<00:12, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 4.48G/9.98G [00:12<00:12, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▌     | 4.53G/9.98G [00:12<00:13, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.58G/9.98G [00:12<00:13, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▋     | 4.63G/9.98G [00:12<00:12, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.69G/9.98G [00:12<00:12, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.74G/9.98G [00:12<00:11, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.79G/9.98G [00:12<00:13, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.83G/9.98G [00:13<00:13, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.89G/9.98G [00:13<00:12, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 4.94G/9.98G [00:13<00:12, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 4.98G/9.98G [00:13<00:13, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 5.02G/9.98G [00:13<00:13, 378MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 5.06G/9.98G [00:13<00:13, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 5.11G/9.98G [00:13<00:13, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.15G/9.98G [00:13<00:13, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.19G/9.98G [00:14<00:13, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.24G/9.98G [00:14<00:12, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.30G/9.98G [00:14<00:11, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▎    | 5.35G/9.98G [00:14<00:11, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.40G/9.98G [00:14<00:10, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 5.45G/9.98G [00:14<00:10, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 5.51G/9.98G [00:14<00:10, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.56G/9.98G [00:14<00:10, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.61G/9.98G [00:14<00:09, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.66G/9.98G [00:15<00:09, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.71G/9.98G [00:15<00:09, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.77G/9.98G [00:15<00:09, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.82G/9.98G [00:15<00:09, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.87G/9.98G [00:15<00:09, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.91G/9.98G [00:15<00:10, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|█████▉    | 5.96G/9.98G [00:15<00:10, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|██████    | 6.00G/9.98G [00:15<00:11, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.04G/9.98G [00:16<00:11, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.09G/9.98G [00:16<00:10, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████▏   | 6.13G/9.98G [00:16<00:10, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.18G/9.98G [00:16<00:10, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.23G/9.98G [00:16<00:09, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.28G/9.98G [00:16<00:09, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.33G/9.98G [00:16<00:08, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.39G/9.98G [00:16<00:08, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▍   | 6.44G/9.98G [00:17<00:08, 428MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 6.49G/9.98G [00:17<00:08, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.54G/9.98G [00:17<00:09, 378MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.59G/9.98G [00:17<00:10, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▋   | 6.63G/9.98G [00:17<00:10, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.67G/9.98G [00:17<00:10, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.72G/9.98G [00:17<00:09, 356MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.77G/9.98G [00:18<00:08, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.82G/9.98G [00:18<00:08, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▊   | 6.86G/9.98G [00:18<00:08, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 6.91G/9.98G [00:18<00:07, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 6.96G/9.98G [00:18<00:07, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|███████   | 7.01G/9.98G [00:18<00:06, 424MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.07G/9.98G [00:18<00:06, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████▏  | 7.12G/9.98G [00:18<00:06, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.17G/9.98G [00:19<00:06, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.22G/9.98G [00:19<00:06, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.27G/9.98G [00:19<00:06, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.31G/9.98G [00:19<00:07, 352MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.36G/9.98G [00:19<00:06, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.41G/9.98G [00:19<00:06, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 7.47G/9.98G [00:19<00:06, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 7.51G/9.98G [00:19<00:06, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.55G/9.98G [00:19<00:06, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.59G/9.98G [00:20<00:06, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.64G/9.98G [00:20<00:08, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.69G/9.98G [00:20<00:08, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.74G/9.98G [00:20<00:07, 316MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.78G/9.98G [00:20<00:06, 334MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.82G/9.98G [00:20<00:06, 346MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.86G/9.98G [00:21<00:05, 357MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.92G/9.98G [00:21<00:05, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 7.96G/9.98G [00:21<00:05, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|████████  | 8.01G/9.98G [00:21<00:04, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.05G/9.98G [00:21<00:05, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.10G/9.98G [00:21<00:05, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.15G/9.98G [00:21<00:04, 386MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.20G/9.98G [00:21<00:04, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.25G/9.98G [00:21<00:04, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.30G/9.98G [00:22<00:03, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.36G/9.98G [00:22<00:03, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.41G/9.98G [00:22<00:03, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▍ | 8.46G/9.98G [00:22<00:03, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▌ | 8.51G/9.98G [00:22<00:03, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.57G/9.98G [00:22<00:03, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▋ | 8.62G/9.98G [00:22<00:03, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.67G/9.98G [00:22<00:03, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.72G/9.98G [00:23<00:02, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.78G/9.98G [00:23<00:02, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.82G/9.98G [00:23<00:02, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.86G/9.98G [00:23<00:02, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [00:23<00:02, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 8.95G/9.98G [00:23<00:02, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|█████████ | 9.00G/9.98G [00:23<00:02, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [00:23<00:02, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.10G/9.98G [00:23<00:02, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.15G/9.98G [00:24<00:02, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.20G/9.98G [00:24<00:01, 399MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.25G/9.98G [00:24<00:01, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.29G/9.98G [00:24<00:01, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▎| 9.34G/9.98G [00:24<00:01, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.40G/9.98G [00:24<00:01, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [00:24<00:01, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▌| 9.49G/9.98G [00:24<00:01, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.54G/9.98G [00:25<00:01, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:25<00:00, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.65G/9.98G [00:25<00:00, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.69G/9.98G [00:25<00:00, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.74G/9.98G [00:25<00:00, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.79G/9.98G [00:25<00:00, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▊| 9.85G/9.98G [00:25<00:00, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.89G/9.98G [00:25<00:00, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|█████████▉| 9.93G/9.98G [00:26<00:00, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|█████████▉| 9.97G/9.98G [00:26<00:00, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:26<00:00, 381MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:26<00:26, 26.25s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 21.0M/3.50G [00:00<00:26, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 41.9M/3.50G [00:00<00:20, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 73.4M/3.50G [00:00<00:15, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 115M/3.50G [00:00<00:11, 282MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 157M/3.50G [00:00<00:10, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 210M/3.50G [00:00<00:09, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 262M/3.50G [00:00<00:08, 378MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▊         | 304M/3.50G [00:00<00:09, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 357M/3.50G [00:01<00:08, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█▏        | 398M/3.50G [00:01<00:08, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 440M/3.50G [00:01<00:09, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 482M/3.50G [00:01<00:09, 322MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 524M/3.50G [00:01<00:08, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▋        | 577M/3.50G [00:01<00:07, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 619M/3.50G [00:01<00:08, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 671M/3.50G [00:01<00:07, 383MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 724M/3.50G [00:02<00:06, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 776M/3.50G [00:02<00:06, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▎       | 828M/3.50G [00:02<00:06, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 881M/3.50G [00:02<00:06, 407MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▋       | 923M/3.50G [00:02<00:06, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 965M/3.50G [00:02<00:06, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 1.01G/3.50G [00:02<00:06, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 1.05G/3.50G [00:02<00:06, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 1.09G/3.50G [00:03<00:06, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 1.13G/3.50G [00:03<00:06, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▎      | 1.17G/3.50G [00:03<00:06, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▌      | 1.23G/3.50G [00:03<00:05, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 1.27G/3.50G [00:03<00:05, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 1.31G/3.50G [00:03<00:05, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▊      | 1.35G/3.50G [00:03<00:05, 389MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 1.41G/3.50G [00:03<00:05, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 1.46G/3.50G [00:03<00:04, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 1.51G/3.50G [00:04<00:04, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 1.56G/3.50G [00:04<00:04, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 1.61G/3.50G [00:04<00:06, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 1.67G/3.50G [00:04<00:05, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 1.72G/3.50G [00:04<00:05, 353MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 1.77G/3.50G [00:04<00:04, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 1.81G/3.50G [00:04<00:04, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 1.87G/3.50G [00:05<00:04, 364MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 1.91G/3.50G [00:05<00:04, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 1.96G/3.50G [00:05<00:03, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 2.01G/3.50G [00:05<00:03, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:05<00:03, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 2.12G/3.50G [00:05<00:03, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 2.17G/3.50G [00:05<00:03, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▎   | 2.22G/3.50G [00:05<00:02, 440MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 2.28G/3.50G [00:06<00:02, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 2.33G/3.50G [00:06<00:02, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 2.38G/3.50G [00:06<00:02, 413MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 2.43G/3.50G [00:06<00:02, 425MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 2.49G/3.50G [00:06<00:02, 432MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:06<00:02, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▎  | 2.58G/3.50G [00:06<00:02, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 2.62G/3.50G [00:06<00:02, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▋  | 2.67G/3.50G [00:07<00:02, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 2.72G/3.50G [00:07<00:02, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 2.76G/3.50G [00:07<00:02, 343MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|████████  | 2.81G/3.50G [00:07<00:01, 354MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:07<00:01, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:07<00:01, 355MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 2.95G/3.50G [00:07<00:01, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▌ | 2.99G/3.50G [00:07<00:01, 390MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 3.03G/3.50G [00:08<00:01, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 3.08G/3.50G [00:08<00:01, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 3.12G/3.50G [00:08<00:00, 376MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:08<00:00, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 3.21G/3.50G [00:08<00:00, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 3.25G/3.50G [00:08<00:00, 369MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 3.30G/3.50G [00:08<00:00, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 3.36G/3.50G [00:08<00:00, 406MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 3.40G/3.50G [00:09<00:00, 370MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▊| 3.45G/3.50G [00:09<00:00, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:09<00:00, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:09<00:00, 377MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:35<00:00, 16.29s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:35<00:00, 17.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 2.32MB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['q_proj', 'v_proj', 'up_proj', 'k_proj', 'gate_proj', 'o_proj', 'down_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 159,907,840 || all params: 3,660,320,768 || trainable%: 4.368683788535114\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: capecape. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: - Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: \\ Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.9\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20230831_142829-huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791-681t4p-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791-681t4p-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/capecape/huggingface\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/capecape/huggingface/runs/huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791-681t4p-algo-1\u001b[0m\n",
      "\u001b[34m0%|          | 0/791 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/791 [00:08<1:46:54,  8.12s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/791 [00:16<1:45:30,  8.02s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/791 [00:24<1:44:58,  7.99s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/791 [00:31<1:44:38,  7.98s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/791 [00:39<1:44:24,  7.97s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/791 [00:47<1:44:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/791 [00:55<1:44:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/791 [01:03<1:43:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 9/791 [01:11<1:43:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 10/791 [01:19<1:43:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6438, 'learning_rate': 0.0001974715549936789, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 10/791 [01:19<1:43:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 11/791 [01:27<1:43:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/791 [01:35<1:43:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/791 [01:43<1:43:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/791 [01:51<1:43:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/791 [01:59<1:42:52,  7.95s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/791 [02:07<1:42:44,  7.95s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 17/791 [02:15<1:42:36,  7.95s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 18/791 [02:23<1:42:28,  7.95s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/791 [02:31<1:42:20,  7.95s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/791 [02:39<1:42:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4619, 'learning_rate': 0.00019494310998735778, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/791 [02:39<1:42:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 21/791 [02:47<1:42:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 22/791 [02:55<1:41:56,  7.95s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 23/791 [03:03<1:41:48,  7.95s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 24/791 [03:11<1:41:40,  7.95s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 25/791 [03:19<1:41:32,  7.95s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 26/791 [03:26<1:41:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 27/791 [03:34<1:41:17,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 28/791 [03:42<1:41:09,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 29/791 [03:50<1:41:01,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 30/791 [03:58<1:40:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4968, 'learning_rate': 0.00019241466498103667, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 30/791 [03:58<1:40:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 31/791 [04:06<1:40:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 32/791 [04:14<1:40:37,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 33/791 [04:22<1:40:29,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 34/791 [04:30<1:40:21,  7.95s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 35/791 [04:38<1:40:13,  7.95s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 36/791 [04:46<1:40:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 37/791 [04:54<1:39:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 38/791 [05:02<1:39:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 39/791 [05:10<1:39:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 40/791 [05:18<1:39:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5377, 'learning_rate': 0.00018988621997471556, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 40/791 [05:18<1:39:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 41/791 [05:26<1:39:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 42/791 [05:34<1:39:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 43/791 [05:42<1:39:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 44/791 [05:50<1:39:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 45/791 [05:58<1:38:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 46/791 [06:06<1:38:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 47/791 [06:14<1:38:38,  7.95s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 48/791 [06:21<1:38:30,  7.95s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 49/791 [06:29<1:38:22,  7.95s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 50/791 [06:37<1:38:14,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4141, 'learning_rate': 0.00018735777496839444, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 50/791 [06:37<1:38:14,  7.95s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 51/791 [06:45<1:38:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 52/791 [06:53<1:37:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 53/791 [07:01<1:37:50,  7.95s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 54/791 [07:09<1:37:42,  7.95s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 55/791 [07:17<1:37:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 56/791 [07:25<1:37:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 57/791 [07:33<1:37:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 58/791 [07:41<1:37:10,  7.95s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 59/791 [07:49<1:37:03,  7.95s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 60/791 [07:57<1:36:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4008, 'learning_rate': 0.00018482932996207333, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 60/791 [07:57<1:36:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 61/791 [08:05<1:36:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 62/791 [08:13<1:36:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 63/791 [08:21<1:36:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 64/791 [08:29<1:36:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 65/791 [08:37<1:36:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 66/791 [08:45<1:36:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 67/791 [08:53<1:35:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 68/791 [09:01<1:35:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 69/791 [09:09<1:35:43,  7.95s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 70/791 [09:17<1:35:35,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3682, 'learning_rate': 0.00018230088495575222, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 70/791 [09:17<1:35:35,  7.95s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 71/791 [09:24<1:35:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 72/791 [09:32<1:35:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 73/791 [09:40<1:35:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 74/791 [09:48<1:35:03,  7.95s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 75/791 [09:56<1:34:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 76/791 [10:04<1:34:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 77/791 [10:12<1:34:39,  7.95s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 78/791 [10:20<1:34:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 79/791 [10:28<1:34:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 80/791 [10:36<1:34:15,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4429, 'learning_rate': 0.0001797724399494311, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 80/791 [10:36<1:34:15,  7.95s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 81/791 [10:44<1:34:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 82/791 [10:52<1:34:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 83/791 [11:00<1:33:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 84/791 [11:08<1:33:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 85/791 [11:16<1:33:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 86/791 [11:24<1:33:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 87/791 [11:32<1:33:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 88/791 [11:40<1:33:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 89/791 [11:48<1:33:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 90/791 [11:56<1:32:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.475, 'learning_rate': 0.00017724399494311, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 90/791 [11:56<1:32:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 91/791 [12:04<1:32:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 92/791 [12:12<1:32:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 93/791 [12:19<1:32:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 94/791 [12:27<1:32:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 95/791 [12:35<1:32:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 96/791 [12:43<1:32:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 97/791 [12:51<1:32:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 98/791 [12:59<1:31:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 99/791 [13:07<1:31:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 100/791 [13:15<1:31:36,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4058, 'learning_rate': 0.00017471554993678888, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 100/791 [13:15<1:31:36,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 101/791 [13:23<1:31:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 102/791 [13:31<1:31:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 103/791 [13:39<1:31:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 104/791 [13:47<1:31:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 105/791 [13:55<1:30:57,  7.95s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 106/791 [14:03<1:30:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 107/791 [14:11<1:30:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 108/791 [14:19<1:30:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 109/791 [14:27<1:30:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 110/791 [14:35<1:30:17,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4557, 'learning_rate': 0.00017218710493046777, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 110/791 [14:35<1:30:17,  7.95s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 111/791 [14:43<1:30:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 112/791 [14:51<1:30:01,  7.95s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 113/791 [14:59<1:29:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 114/791 [15:07<1:29:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 115/791 [15:14<1:29:37,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 116/791 [15:22<1:29:29,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 117/791 [15:30<1:29:21,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 118/791 [15:38<1:29:13,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 119/791 [15:46<1:29:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 120/791 [15:54<1:28:57,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3752, 'learning_rate': 0.00016965865992414668, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 120/791 [15:54<1:28:57,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 121/791 [16:02<1:28:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 122/791 [16:10<1:28:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 123/791 [16:18<1:28:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 124/791 [16:26<1:28:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 125/791 [16:34<1:28:17,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 126/791 [16:42<1:28:09,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 127/791 [16:50<1:28:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 128/791 [16:58<1:27:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 129/791 [17:06<1:27:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 130/791 [17:14<1:27:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3495, 'learning_rate': 0.00016713021491782554, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 130/791 [17:14<1:27:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 131/791 [17:22<1:27:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 132/791 [17:30<1:27:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 133/791 [17:38<1:27:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 134/791 [17:46<1:27:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 135/791 [17:54<1:26:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 136/791 [18:02<1:26:50,  7.95s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 137/791 [18:09<1:26:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 138/791 [18:17<1:26:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 139/791 [18:25<1:26:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 140/791 [18:33<1:26:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3707, 'learning_rate': 0.00016460176991150443, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 140/791 [18:33<1:26:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 141/791 [18:41<1:26:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 142/791 [18:49<1:26:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 143/791 [18:57<1:25:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 144/791 [19:05<1:25:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 145/791 [19:13<1:25:38,  7.95s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 146/791 [19:21<1:25:30,  7.95s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 147/791 [19:29<1:25:22,  7.95s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 148/791 [19:37<1:25:14,  7.95s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 149/791 [19:45<1:25:06,  7.95s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 150/791 [19:53<1:24:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3725, 'learning_rate': 0.00016207332490518332, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 150/791 [19:53<1:24:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 151/791 [20:01<1:24:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 152/791 [20:09<1:24:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 153/791 [20:17<1:24:35,  7.95s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 154/791 [20:25<1:24:27,  7.95s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 155/791 [20:33<1:24:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 156/791 [20:41<1:24:11,  7.95s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 157/791 [20:49<1:24:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 158/791 [20:57<1:23:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 159/791 [21:05<1:23:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 160/791 [21:12<1:23:39,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4125, 'learning_rate': 0.0001595448798988622, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 160/791 [21:12<1:23:39,  7.95s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 161/791 [21:20<1:23:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 162/791 [21:28<1:23:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 163/791 [21:36<1:23:15,  7.95s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 164/791 [21:44<1:23:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 165/791 [21:52<1:22:59,  7.95s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 166/791 [22:00<1:22:51,  7.95s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 167/791 [22:08<1:22:43,  7.95s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 168/791 [22:16<1:22:35,  7.95s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 169/791 [22:24<1:22:27,  7.95s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 170/791 [22:32<1:22:19,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3227, 'learning_rate': 0.0001570164348925411, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 170/791 [22:32<1:22:19,  7.95s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 171/791 [22:40<1:22:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 172/791 [22:48<1:22:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 173/791 [22:56<1:21:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 174/791 [23:04<1:21:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 175/791 [23:12<1:21:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 176/791 [23:20<1:21:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 177/791 [23:28<1:21:24,  7.95s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 178/791 [23:36<1:21:16,  7.95s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 179/791 [23:44<1:21:08,  7.95s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 180/791 [23:52<1:21:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4149, 'learning_rate': 0.00015448798988621998, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 180/791 [23:52<1:21:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 181/791 [24:00<1:20:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 182/791 [24:07<1:20:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 183/791 [24:15<1:20:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 184/791 [24:23<1:20:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 185/791 [24:31<1:20:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 186/791 [24:39<1:20:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 187/791 [24:47<1:20:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 188/791 [24:55<1:19:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 189/791 [25:03<1:19:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 190/791 [25:11<1:19:40,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3301, 'learning_rate': 0.00015195954487989886, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 190/791 [25:11<1:19:40,  7.95s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 191/791 [25:19<1:19:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 192/791 [25:27<1:19:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 193/791 [25:35<1:19:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 194/791 [25:43<1:19:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 195/791 [25:51<1:19:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 196/791 [25:59<1:18:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 197/791 [26:07<1:18:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 198/791 [26:15<1:18:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 199/791 [26:23<1:18:29,  7.95s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 200/791 [26:31<1:18:21,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4319, 'learning_rate': 0.00014943109987357775, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 200/791 [26:31<1:18:21,  7.95s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 201/791 [26:39<1:18:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 202/791 [26:47<1:18:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 203/791 [26:55<1:17:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 204/791 [27:02<1:17:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 205/791 [27:10<1:17:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 206/791 [27:18<1:17:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 207/791 [27:26<1:17:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 208/791 [27:34<1:17:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 209/791 [27:42<1:17:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 210/791 [27:50<1:17:01,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4302, 'learning_rate': 0.00014690265486725664, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 210/791 [27:50<1:17:01,  7.95s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 211/791 [27:58<1:16:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 212/791 [28:06<1:16:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 213/791 [28:14<1:16:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 214/791 [28:22<1:16:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 215/791 [28:30<1:16:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 216/791 [28:38<1:16:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 217/791 [28:46<1:16:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 218/791 [28:54<1:15:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 219/791 [29:02<1:15:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 220/791 [29:10<1:15:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3753, 'learning_rate': 0.00014437420986093552, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 220/791 [29:10<1:15:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 221/791 [29:18<1:15:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 222/791 [29:26<1:15:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 223/791 [29:34<1:15:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 224/791 [29:42<1:15:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 225/791 [29:50<1:15:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 226/791 [29:58<1:14:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 227/791 [30:05<1:14:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 228/791 [30:13<1:14:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 229/791 [30:21<1:14:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 230/791 [30:29<1:14:22,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3366, 'learning_rate': 0.00014184576485461444, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 230/791 [30:29<1:14:22,  7.95s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 231/791 [30:37<1:14:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 232/791 [30:45<1:14:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 233/791 [30:53<1:13:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 234/791 [31:01<1:13:50,  7.95s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 235/791 [31:09<1:13:42,  7.95s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 236/791 [31:17<1:13:35,  7.95s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 237/791 [31:25<1:13:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 238/791 [31:33<1:13:19,  7.95s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 239/791 [31:41<1:13:11,  7.95s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 240/791 [31:49<1:13:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4281, 'learning_rate': 0.0001393173198482933, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 240/791 [31:49<1:13:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 241/791 [31:57<1:12:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 242/791 [32:05<1:12:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 243/791 [32:13<1:12:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 244/791 [32:21<1:12:31,  7.95s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 245/791 [32:29<1:12:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 246/791 [32:37<1:12:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 247/791 [32:45<1:12:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 248/791 [32:53<1:11:59,  7.95s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 249/791 [33:00<1:11:51,  7.95s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 250/791 [33:08<1:11:43,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3431, 'learning_rate': 0.0001367888748419722, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 250/791 [33:08<1:11:43,  7.95s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 251/791 [33:16<1:11:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 252/791 [33:24<1:11:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 253/791 [33:32<1:11:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 254/791 [33:40<1:11:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 255/791 [33:48<1:11:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 256/791 [33:56<1:10:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 257/791 [34:04<1:10:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 258/791 [34:12<1:10:39,  7.95s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 259/791 [34:20<1:10:31,  7.95s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 260/791 [34:28<1:10:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4121, 'learning_rate': 0.00013426042983565107, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 260/791 [34:28<1:10:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 261/791 [34:36<1:10:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 262/791 [34:44<1:10:08,  7.95s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 263/791 [34:52<1:10:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 264/791 [35:00<1:09:52,  7.95s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 265/791 [35:08<1:09:44,  7.95s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 266/791 [35:16<1:09:36,  7.95s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 267/791 [35:24<1:09:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 268/791 [35:32<1:09:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 269/791 [35:40<1:09:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 270/791 [35:48<1:09:04,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3837, 'learning_rate': 0.00013173198482932996, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 270/791 [35:48<1:09:04,  7.95s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 271/791 [35:55<1:08:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 272/791 [36:03<1:08:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 273/791 [36:11<1:08:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 274/791 [36:19<1:08:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 275/791 [36:27<1:08:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 276/791 [36:35<1:08:16,  7.95s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 277/791 [36:43<1:08:08,  7.95s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 278/791 [36:51<1:08:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 279/791 [36:59<1:07:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 280/791 [37:07<1:07:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5345, 'learning_rate': 0.00012920353982300885, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 280/791 [37:07<1:07:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 281/791 [37:15<1:07:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 282/791 [37:23<1:07:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 283/791 [37:31<1:07:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 284/791 [37:39<1:07:13,  7.95s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 285/791 [37:47<1:07:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 286/791 [37:55<1:06:57,  7.95s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 287/791 [38:03<1:06:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 288/791 [38:11<1:06:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 289/791 [38:19<1:06:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 290/791 [38:27<1:06:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.456, 'learning_rate': 0.00012667509481668773, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 290/791 [38:27<1:06:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 291/791 [38:35<1:06:17,  7.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 292/791 [38:43<1:06:09,  7.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 293/791 [38:50<1:06:01,  7.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 294/791 [38:58<1:05:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 295/791 [39:06<1:05:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 296/791 [39:14<1:05:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 297/791 [39:22<1:05:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 298/791 [39:30<1:05:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 299/791 [39:38<1:05:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 300/791 [39:46<1:05:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.433, 'learning_rate': 0.00012414664981036662, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 300/791 [39:46<1:05:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 301/791 [39:54<1:04:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 302/791 [40:02<1:04:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 303/791 [40:10<1:04:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 304/791 [40:18<1:04:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 305/791 [40:26<1:04:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 306/791 [40:34<1:04:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 307/791 [40:42<1:04:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 308/791 [40:50<1:04:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 309/791 [40:58<1:03:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 310/791 [41:06<1:03:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4461, 'learning_rate': 0.00012161820480404551, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 310/791 [41:06<1:03:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 311/791 [41:14<1:03:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 312/791 [41:22<1:03:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 313/791 [41:30<1:03:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 314/791 [41:38<1:03:14,  7.95s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 315/791 [41:45<1:03:06,  7.95s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 316/791 [41:53<1:02:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 317/791 [42:01<1:02:50,  7.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 318/791 [42:09<1:02:42,  7.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 319/791 [42:17<1:02:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 320/791 [42:25<1:02:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3903, 'learning_rate': 0.00011908975979772441, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 320/791 [42:25<1:02:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 321/791 [42:33<1:02:18,  7.96s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 322/791 [42:41<1:02:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 323/791 [42:49<1:02:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 324/791 [42:57<1:01:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 325/791 [43:05<1:01:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 326/791 [43:13<1:01:38,  7.95s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 327/791 [43:21<1:01:30,  7.95s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 328/791 [43:29<1:01:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 329/791 [43:37<1:01:15,  7.95s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 330/791 [43:45<1:01:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4555, 'learning_rate': 0.00011656131479140328, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 330/791 [43:45<1:01:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 331/791 [43:53<1:00:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 332/791 [44:01<1:00:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 333/791 [44:09<1:00:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 334/791 [44:17<1:00:35,  7.95s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 335/791 [44:25<1:00:27,  7.95s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 336/791 [44:33<1:00:19,  7.95s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 337/791 [44:41<1:00:11,  7.95s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 338/791 [44:48<1:00:03,  7.95s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 339/791 [44:56<59:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 340/791 [45:04<59:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.406, 'learning_rate': 0.00011403286978508218, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 340/791 [45:04<59:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 341/791 [45:12<59:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 342/791 [45:20<59:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 343/791 [45:28<59:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 344/791 [45:36<59:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 345/791 [45:44<59:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 346/791 [45:52<59:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 347/791 [46:00<58:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 348/791 [46:08<58:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 349/791 [46:16<58:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 350/791 [46:24<58:28,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.465, 'learning_rate': 0.00011150442477876106, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 350/791 [46:24<58:28,  7.95s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 351/791 [46:32<58:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 352/791 [46:40<58:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 353/791 [46:48<58:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 354/791 [46:56<57:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 355/791 [47:04<57:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 356/791 [47:12<57:40,  7.95s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 357/791 [47:20<57:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 358/791 [47:28<57:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 359/791 [47:36<57:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 360/791 [47:43<57:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5195, 'learning_rate': 0.00010897597977243996, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 360/791 [47:43<57:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 361/791 [47:51<57:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 362/791 [47:59<56:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 363/791 [48:07<56:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 364/791 [48:15<56:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 365/791 [48:23<56:28,  7.95s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 366/791 [48:31<56:20,  7.95s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 367/791 [48:39<56:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 368/791 [48:47<56:04,  7.95s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 369/791 [48:55<55:56,  7.95s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 370/791 [49:03<55:48,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4427, 'learning_rate': 0.00010644753476611884, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 370/791 [49:03<55:48,  7.95s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 371/791 [49:11<55:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 372/791 [49:19<55:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 373/791 [49:27<55:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 374/791 [49:35<55:17,  7.95s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 375/791 [49:43<55:09,  7.95s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 376/791 [49:51<55:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 377/791 [49:59<54:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 378/791 [50:07<54:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 379/791 [50:15<54:37,  7.95s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 380/791 [50:23<54:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3101, 'learning_rate': 0.00010391908975979774, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 380/791 [50:23<54:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 381/791 [50:31<54:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 382/791 [50:38<54:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 383/791 [50:46<54:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 384/791 [50:54<53:57,  7.95s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 385/791 [51:02<53:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 386/791 [51:10<53:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 387/791 [51:18<53:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 388/791 [51:26<53:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 389/791 [51:34<53:17,  7.95s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 390/791 [51:42<53:09,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3003, 'learning_rate': 0.00010139064475347662, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 390/791 [51:42<53:09,  7.95s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 391/791 [51:50<53:01,  7.95s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 392/791 [51:58<52:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 393/791 [52:06<52:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 394/791 [52:14<52:37,  7.95s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 395/791 [52:22<52:30,  7.95s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 396/791 [52:30<52:22,  7.95s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 397/791 [52:38<52:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 398/791 [52:46<52:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 399/791 [52:54<51:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 400/791 [53:02<51:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4762, 'learning_rate': 9.88621997471555e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 400/791 [53:02<51:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 401/791 [53:10<51:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 402/791 [53:18<51:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 403/791 [53:26<51:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 404/791 [53:33<51:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 405/791 [53:41<51:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 406/791 [53:49<51:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 407/791 [53:57<50:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 408/791 [54:05<50:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 409/791 [54:13<50:38,  7.95s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 410/791 [54:21<50:30,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4426, 'learning_rate': 9.633375474083439e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 410/791 [54:21<50:30,  7.95s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 411/791 [54:29<50:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 412/791 [54:37<50:14,  7.95s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 413/791 [54:45<50:06,  7.95s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 414/791 [54:53<49:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 415/791 [55:01<49:50,  7.95s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 416/791 [55:09<49:42,  7.95s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 417/791 [55:17<49:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 418/791 [55:25<49:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 419/791 [55:33<49:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 420/791 [55:41<49:10,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.26, 'learning_rate': 9.380530973451328e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 420/791 [55:41<49:10,  7.95s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 421/791 [55:49<49:03,  7.95s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 422/791 [55:57<48:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 423/791 [56:05<48:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 424/791 [56:13<48:39,  7.95s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 425/791 [56:21<48:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 426/791 [56:28<48:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 427/791 [56:36<48:15,  7.95s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 428/791 [56:44<48:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 429/791 [56:52<47:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 430/791 [57:00<47:51,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3458, 'learning_rate': 9.127686472819217e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 430/791 [57:00<47:51,  7.95s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 431/791 [57:08<47:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 432/791 [57:16<47:35,  7.96s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 433/791 [57:24<47:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 434/791 [57:32<47:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 435/791 [57:40<47:11,  7.95s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 436/791 [57:48<47:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 437/791 [57:56<46:56,  7.95s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 438/791 [58:04<46:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 439/791 [58:12<46:39,  7.95s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 440/791 [58:20<46:32,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3961, 'learning_rate': 8.874841972187105e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 440/791 [58:20<46:32,  7.95s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 441/791 [58:28<46:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 442/791 [58:36<46:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 443/791 [58:44<46:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 444/791 [58:52<46:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 445/791 [59:00<45:52,  7.95s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 446/791 [59:08<45:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 447/791 [59:16<45:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 448/791 [59:24<45:28,  7.95s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 449/791 [59:31<45:20,  7.95s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 450/791 [59:39<45:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5095, 'learning_rate': 8.621997471554994e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 450/791 [59:39<45:12,  7.95s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 451/791 [59:47<45:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 452/791 [59:55<44:56,  7.95s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 453/791 [1:00:03<44:48,  7.95s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 454/791 [1:00:11<44:40,  7.95s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 455/791 [1:00:19<44:32,  7.95s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 456/791 [1:00:27<44:24,  7.95s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 457/791 [1:00:35<44:16,  7.95s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 458/791 [1:00:43<44:08,  7.95s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 459/791 [1:00:51<44:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 460/791 [1:00:59<43:52,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3512, 'learning_rate': 8.369152970922883e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 460/791 [1:00:59<43:52,  7.95s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 461/791 [1:01:07<43:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 462/791 [1:01:15<43:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 463/791 [1:01:23<43:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 464/791 [1:01:31<43:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 465/791 [1:01:39<43:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 466/791 [1:01:47<43:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 467/791 [1:01:55<42:57,  7.95s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 468/791 [1:02:03<42:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 469/791 [1:02:11<42:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 470/791 [1:02:19<42:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3791, 'learning_rate': 8.116308470290771e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 470/791 [1:02:19<42:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 471/791 [1:02:26<42:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 472/791 [1:02:34<42:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 473/791 [1:02:42<42:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 474/791 [1:02:50<42:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 475/791 [1:02:58<41:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 476/791 [1:03:06<41:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 477/791 [1:03:14<41:37,  7.95s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 478/791 [1:03:22<41:29,  7.95s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 479/791 [1:03:30<41:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 480/791 [1:03:38<41:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3229, 'learning_rate': 7.86346396965866e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 480/791 [1:03:38<41:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 481/791 [1:03:46<41:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 482/791 [1:03:54<40:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 483/791 [1:04:02<40:50,  7.95s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 484/791 [1:04:10<40:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 485/791 [1:04:18<40:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 486/791 [1:04:26<40:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 487/791 [1:04:34<40:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 488/791 [1:04:42<40:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 489/791 [1:04:50<40:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 490/791 [1:04:58<39:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3574, 'learning_rate': 7.610619469026549e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 490/791 [1:04:58<39:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 491/791 [1:05:06<39:47,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 492/791 [1:05:14<39:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 493/791 [1:05:21<39:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 494/791 [1:05:29<39:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 495/791 [1:05:37<39:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 496/791 [1:05:45<39:06,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 497/791 [1:05:53<38:58,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 498/791 [1:06:01<38:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 499/791 [1:06:09<38:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 500/791 [1:06:17<38:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.279, 'learning_rate': 7.357774968394438e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 500/791 [1:06:17<38:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 501/791 [1:06:25<38:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 502/791 [1:06:33<38:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 503/791 [1:06:41<38:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 504/791 [1:06:49<38:03,  7.95s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 505/791 [1:06:57<37:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 506/791 [1:07:05<37:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 507/791 [1:07:13<37:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 508/791 [1:07:21<37:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 509/791 [1:07:29<37:23,  7.96s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 510/791 [1:07:37<37:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4068, 'learning_rate': 7.104930467762326e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 510/791 [1:07:37<37:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 511/791 [1:07:45<37:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 512/791 [1:07:53<36:59,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 513/791 [1:08:01<36:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 514/791 [1:08:09<36:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 515/791 [1:08:17<36:35,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 516/791 [1:08:24<36:27,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 517/791 [1:08:32<36:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 518/791 [1:08:40<36:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 519/791 [1:08:48<36:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 520/791 [1:08:56<35:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3975, 'learning_rate': 6.852085967130215e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 520/791 [1:08:56<35:55,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 521/791 [1:09:04<35:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 522/791 [1:09:12<35:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 523/791 [1:09:20<35:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 524/791 [1:09:28<35:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 525/791 [1:09:36<35:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 526/791 [1:09:44<35:08,  7.95s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 527/791 [1:09:52<35:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 528/791 [1:10:00<34:52,  7.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 529/791 [1:10:08<34:44,  7.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 530/791 [1:10:16<34:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3767, 'learning_rate': 6.599241466498104e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 530/791 [1:10:16<34:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 531/791 [1:10:24<34:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 532/791 [1:10:32<34:20,  7.96s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 533/791 [1:10:40<34:12,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 534/791 [1:10:48<34:04,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 535/791 [1:10:56<33:56,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 536/791 [1:11:04<33:48,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 537/791 [1:11:12<33:40,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 538/791 [1:11:19<33:32,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 539/791 [1:11:27<33:24,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 540/791 [1:11:35<33:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3825, 'learning_rate': 6.346396965865992e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 540/791 [1:11:35<33:16,  7.96s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 541/791 [1:11:43<33:08,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 542/791 [1:11:51<33:00,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 543/791 [1:11:59<32:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 544/791 [1:12:07<32:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 545/791 [1:12:15<32:36,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 546/791 [1:12:23<32:28,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 547/791 [1:12:31<32:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 548/791 [1:12:39<32:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 549/791 [1:12:47<32:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 550/791 [1:12:55<31:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4195, 'learning_rate': 6.093552465233882e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 550/791 [1:12:55<31:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 551/791 [1:13:03<31:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 552/791 [1:13:11<31:41,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 553/791 [1:13:19<31:33,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 554/791 [1:13:27<31:25,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 555/791 [1:13:35<31:17,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 556/791 [1:13:43<31:09,  7.96s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 557/791 [1:13:51<31:01,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 558/791 [1:13:59<30:53,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 559/791 [1:14:07<30:45,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 560/791 [1:14:14<30:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4178, 'learning_rate': 5.8407079646017705e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 560/791 [1:14:15<30:37,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 561/791 [1:14:22<30:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 562/791 [1:14:30<30:21,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 563/791 [1:14:38<30:13,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 564/791 [1:14:46<30:05,  7.96s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 565/791 [1:14:54<29:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 566/791 [1:15:02<29:49,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 567/791 [1:15:10<29:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 568/791 [1:15:18<29:34,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 569/791 [1:15:26<29:26,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 570/791 [1:15:34<29:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3436, 'learning_rate': 5.587863463969659e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 570/791 [1:15:34<29:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 571/791 [1:15:42<29:10,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 572/791 [1:15:50<29:02,  7.96s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 573/791 [1:15:58<28:54,  7.96s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 574/791 [1:16:06<28:46,  7.96s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 575/791 [1:16:14<28:38,  7.96s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 576/791 [1:16:22<28:30,  7.96s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 577/791 [1:16:30<28:22,  7.96s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 578/791 [1:16:38<28:14,  7.96s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 579/791 [1:16:46<28:06,  7.95s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 580/791 [1:16:54<27:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3105, 'learning_rate': 5.335018963337548e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 580/791 [1:16:54<27:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 581/791 [1:17:02<27:50,  7.96s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 582/791 [1:17:10<27:42,  7.96s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 583/791 [1:17:17<27:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 584/791 [1:17:25<27:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 585/791 [1:17:33<27:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 586/791 [1:17:41<27:10,  7.95s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 587/791 [1:17:49<27:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 588/791 [1:17:57<26:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 589/791 [1:18:05<26:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 590/791 [1:18:13<26:38,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4197, 'learning_rate': 5.0821744627054366e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 590/791 [1:18:13<26:38,  7.95s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 591/791 [1:18:21<26:31,  7.96s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 592/791 [1:18:29<26:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 593/791 [1:18:37<26:15,  7.96s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 594/791 [1:18:45<26:07,  7.96s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 595/791 [1:18:53<25:59,  7.95s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 596/791 [1:19:01<25:51,  7.96s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 597/791 [1:19:09<25:43,  7.96s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 598/791 [1:19:17<25:35,  7.96s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 599/791 [1:19:25<25:27,  7.95s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 600/791 [1:19:33<25:19,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4152, 'learning_rate': 4.829329962073325e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 600/791 [1:19:33<25:19,  7.95s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 601/791 [1:19:41<25:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 602/791 [1:19:49<25:03,  7.96s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 603/791 [1:19:57<24:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 604/791 [1:20:05<24:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 605/791 [1:20:12<24:39,  7.95s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 729/791 [1:36:39<08:13,  7.95s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 730/791 [1:36:47<08:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3662, 'learning_rate': 1.5423514538558785e-05, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 730/791 [1:36:47<08:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 731/791 [1:36:55<07:57,  7.96s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 732/791 [1:37:03<07:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 733/791 [1:37:11<07:41,  7.95s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 734/791 [1:37:19<07:33,  7.95s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 735/791 [1:37:27<07:25,  7.95s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 736/791 [1:37:35<07:17,  7.95s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 737/791 [1:37:43<07:09,  7.95s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 738/791 [1:37:50<07:01,  7.95s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 739/791 [1:37:58<06:53,  7.95s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 740/791 [1:38:06<06:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3622, 'learning_rate': 1.2895069532237675e-05, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 740/791 [1:38:06<06:45,  7.95s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 741/791 [1:38:14<06:37,  7.95s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 742/791 [1:38:22<06:29,  7.95s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 743/791 [1:38:30<06:21,  7.95s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 744/791 [1:38:38<06:13,  7.95s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 745/791 [1:38:46<06:05,  7.95s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 746/791 [1:38:54<05:57,  7.95s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 747/791 [1:39:02<05:49,  7.95s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 748/791 [1:39:10<05:42,  7.95s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 749/791 [1:39:18<05:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 750/791 [1:39:26<05:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4352, 'learning_rate': 1.036662452591656e-05, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 750/791 [1:39:26<05:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 751/791 [1:39:34<05:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 752/791 [1:39:42<05:10,  7.95s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 753/791 [1:39:50<05:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 754/791 [1:39:58<04:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 755/791 [1:40:06<04:46,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 756/791 [1:40:14<04:38,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 757/791 [1:40:22<04:30,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 758/791 [1:40:30<04:22,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 759/791 [1:40:38<04:14,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 760/791 [1:40:45<04:06,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4711, 'learning_rate': 7.83817951959545e-06, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 760/791 [1:40:45<04:06,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 761/791 [1:40:53<03:58,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 762/791 [1:41:01<03:50,  7.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 763/791 [1:41:09<03:42,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 764/791 [1:41:17<03:34,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 765/791 [1:41:25<03:26,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 766/791 [1:41:33<03:18,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 767/791 [1:41:41<03:10,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 768/791 [1:41:49<03:02,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 769/791 [1:41:57<02:54,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 770/791 [1:42:05<02:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3608, 'learning_rate': 5.3097345132743365e-06, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 770/791 [1:42:05<02:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 771/791 [1:42:13<02:39,  7.96s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 772/791 [1:42:21<02:31,  7.95s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 773/791 [1:42:29<02:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 774/791 [1:42:37<02:15,  7.95s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 775/791 [1:42:45<02:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 776/791 [1:42:53<01:59,  7.95s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 777/791 [1:43:01<01:51,  7.95s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 778/791 [1:43:09<01:43,  7.95s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 779/791 [1:43:17<01:35,  7.95s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 780/791 [1:43:25<01:27,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4426, 'learning_rate': 2.781289506953224e-06, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 780/791 [1:43:25<01:27,  7.95s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 781/791 [1:43:33<01:19,  7.96s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 782/791 [1:43:40<01:11,  7.96s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 783/791 [1:43:48<01:03,  7.95s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 784/791 [1:43:56<00:55,  7.95s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 785/791 [1:44:04<00:47,  7.95s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 786/791 [1:44:12<00:39,  7.95s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 787/791 [1:44:20<00:31,  7.95s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 788/791 [1:44:28<00:23,  7.95s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 789/791 [1:44:36<00:15,  7.95s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 790/791 [1:44:44<00:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3807, 'learning_rate': 2.528445006321113e-07, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 790/791 [1:44:44<00:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 791/791 [1:44:48<00:00,  6.82s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 6300.7363, 'train_samples_per_second': 0.251, 'train_steps_per_second': 0.126, 'train_loss': 1.40276567688784, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 791/791 [1:44:48<00:00,  6.82s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 791/791 [1:44:48<00:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  6.07it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.66it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.82it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 4.95MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 153MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 30.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.98MB/s]\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\u001b[0m\n",
      "\u001b[34mwandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\u001b[0m\n",
      "\u001b[34mwandb:            train/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\u001b[0m\n",
      "\u001b[34mwandb:                     train/loss █▅▃▂▅▄▂▂▁▂▄▂▂▃▄▄▄▄▄▁▄▂▅▃▂▃▃▃▂▃▃▁▂▃▂▂▂▄▂▃\u001b[0m\n",
      "\u001b[34mwandb:               train/total_flos ▁\u001b[0m\n",
      "\u001b[34mwandb:               train/train_loss ▁\u001b[0m\n",
      "\u001b[34mwandb:            train/train_runtime ▁\u001b[0m\n",
      "\u001b[34mwandb: train/train_samples_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:   train/train_steps_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:                    train/epoch 1.0\u001b[0m\n",
      "\u001b[34mwandb:              train/global_step 791\u001b[0m\n",
      "\u001b[34mwandb:            train/learning_rate 0.0\u001b[0m\n",
      "\u001b[34mwandb:                     train/loss 1.3807\u001b[0m\n",
      "\u001b[34mwandb:               train/total_flos 6.85638734095319e+16\u001b[0m\n",
      "\u001b[34mwandb:               train/train_loss 1.40277\u001b[0m\n",
      "\u001b[34mwandb:            train/train_runtime 6300.7363\u001b[0m\n",
      "\u001b[34mwandb: train/train_samples_per_second 0.251\u001b[0m\n",
      "\u001b[34mwandb:   train/train_steps_per_second 0.126\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791-681t4p-algo-1 at: https://wandb.ai/capecape/huggingface/runs/huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791-681t4p-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20230831_142829-huggingface-qlora-2023-08-31-14-21-10-2023-08-31-14-21-12-791-681t4p-algo-1/logs\u001b[0m\n",
      "\u001b[34m2023-08-31 16:16:28,890 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-31 16:16:28,890 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-31 16:16:28,890 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-08-31 16:16:32 Uploading - Uploading generated training model"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for LLaMA 13B, the SageMaker training job took `31728 seconds`, which is about `8.8 hours`. The ml.g5.4xlarge instance we used costs `$2.03 per hour` for on-demand usage. As a result, the total cost for training our fine-tuned LLaMa 2 model was only ~`$18`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "\n",
    "You can deploy your fine-tuned LLaMA model to a SageMaker endpoint and use it for inference. Check out the [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm) and [Securely deploy LLMs inside VPCs with Hugging Face and Amazon SageMaker](https://www.philschmid.de/sagemaker-llm-vpc) for more details."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-cpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
